# Chat With PDF using Ollama and LangChain
This is a simple LLM project I made which allows users to upload their PDF file and interact/query its contents using a local LLM through Ollama. The model uses Retrieval Augemented Generation (RAG) to provide relevant answers based on the PDF file's content.

## Key Features
- Upload a PDF document file.
- Ask questions about the PDF file's content.
- Uses vector similarity to retrieve the most relevant information.
- Answers are generated by local LLM, Llama3.
- Simple web UI is provided.
- Added information on extracted texts, number of chunks created and relevant part of the file based on query for better understanding of the system.

## Tools and Libraries Used
- Python - The main programming language
- Ollama - A local LLM runner. We used the Llama3 model in this project.
- LangChain - For chain logic and RAG setup.
- PyPDF - Allows PDF file text extraction.
- FAISS - A vector database for similarity search
- HuggingFace Embedddings - To convert text to vector representations
- Streamlit - Enables a simple web UI

## Running the Project
> [!NOTE]
> Make sure you have installed Ollama into your device and run Llama3 before proceeding with the next steps.

### 1. Set up Virtual Environment

'''bash
python -m venv .venv
.venv\Scripts\activate # for Windows, source venv/bin/activate for macOS
'''

### 2. Install Requirements

'''bash
pip install -r requirements.txt
'''

### 3. Run the App

'''bash
streamlit run app.py
'''